{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4415676b-89fb-42ee-8c27-2a033e5755e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "                        MSE        R²\n",
      "Linear Regression  0.204590  0.968755\n",
      "Ridge Regression   0.204648  0.968746\n",
      "Lasso Regression   1.941367  0.703512\n",
      "Best Performing Model: Linear Regression\n",
      "Reason: Linear Regression achieved the highest R² is because the dataset isn't very high-dimensional leading to underfitting (Especially in lasso)\n",
      "Ridge and Linear are very close to each other, indicating the L2 penalty is very small.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "file_path=\"./mental_health_workplace_survey.csv\"\n",
    "data=pd.read_csv(file_path)  \n",
    "\n",
    "def remove_outliers_iqr(data, col):\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]\n",
    "\n",
    "for col in ['WorkHoursPerWeek', 'SleepHours', 'StressLevel']:\n",
    "    data = remove_outliers_iqr(data, col)\n",
    "\n",
    "data['Stress_WorkHours'] = data['StressLevel'] * data['WorkHoursPerWeek']\n",
    "data['Sleep_vs_Stress'] = data['SleepHours'] / (data['StressLevel'] + 1)\n",
    "\n",
    "X = data.drop([\"StressLevel\",\"EmployeeID\",\"BurnoutRisk\"], axis=1)\n",
    "y = data[\"StressLevel\"]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid=train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "    \n",
    "categorical_features = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "for col in categorical_features:\n",
    "    data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "\n",
    "numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "for col in numerical_features:\n",
    "    data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# Fill missing values (if not already done)\n",
    "for col in categorical_features:\n",
    "    data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "\n",
    "# Ordinal Encoding\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_train[categorical_features] = ordinal_encoder.fit_transform(X_train[categorical_features])\n",
    "X_valid[categorical_features] = ordinal_encoder.transform(X_valid[categorical_features])\n",
    "\n",
    "# Standardize the features cuz models like Ridge and Lasso Regression are sensitive to the size of the features.\n",
    "#Helps in maintaining the contibution by each feature. Feature Scaling is not always required for Linear Regression models.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Feature selection using Mutual Information\n",
    "selector = SelectKBest(score_func=mutual_info_regression,k=24) #Choosing all 24 features cuz it's already a small enough dataset\n",
    "selector.fit(X_train_scaled, y_train) #Decreasing it more will decrease the R²\n",
    "\n",
    "selected_features_mask = selector.get_support()\n",
    "selected_features = X_train.columns[selected_features_mask]\n",
    "\n",
    "# Filter features\n",
    "X_train_selected = X_train_scaled[:, selected_features_mask]\n",
    "X_valid_selected = X_valid_scaled[:, selected_features_mask]\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train) #X_train_scaled\n",
    "    y_pred = model.predict(X_valid_selected) #X_valid_scaled\n",
    "    mse = mean_squared_error(y_valid, y_pred)\n",
    "    r2 = r2_score(y_valid, y_pred)\n",
    "    results[name] = {'MSE': mse, 'R²': r2}\n",
    "\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "results_data = pd.DataFrame(results).T\n",
    "print(results_data)\n",
    "\n",
    "best_model = results_data['R²'].idxmax()\n",
    "print(f\"Best Performing Model: {best_model}\")\n",
    "print(f\"Reason: {best_model} achieved the highest R² is because the dataset isn't very high-dimensional leading to underfitting (Especially in lasso)\")\n",
    "print(\"Ridge and Linear are very close to each other, indicating the L2 penalty is very small.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
